import os, html, textwrap, sys
import requests, feedparser
from urllib.parse import urlparse, urlunparse
from datetime import datetime, timezone
from dateutil import parser as dtparse
import argparse
import time

# ì‚¬ìš©ë²•
#
# # 1) ê°€ì¥ ì˜¤ë˜ëœ ê¸€ë¶€í„° 5ê°œ ë°±í•„
# python backfill_blogger.py --max 5 --oldest-first
#
# # 2) ìµœì‹ ê¸€ 10ê°œëŠ” ê±´ë„ˆë›°ê³  ê·¸ ë‹¤ìŒ 20ê°œ ë°±í•„
# python backfill_blogger.py --skip 10 --max 20 --oldest-first
#
# # 3) ë‚ ì§œ ë²”ìœ„ë¡œ ë°±í•„ (ì˜ˆ: 2024-01-01 ~ 2024-12-31)
# python backfill_blogger.py --since 2024-01-01 --until 2024-12-31 --oldest-first
#
# # 4) ì´ë¯¸ ì˜¬ë¦° ê¸€ì´ë¼ë„ ê°•ì œë¡œ ë‹¤ì‹œ ì˜¬ë¦¬ê¸°
# python backfill_blogger.py --max 3 --force
#
# # 5) ë¦¬í—ˆì„¤(ì‹¤ì œ ì—…ë¡œë“œ X)
# python backfill_blogger.py --max 10 --dry-run


# ===== í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ =====
CLIENT_ID      = os.environ["GCP_CLIENT_ID"]
CLIENT_SECRET  = os.environ["GCP_CLIENT_SECRET"]
REFRESH_TOKEN  = os.environ["GCP_REFRESH_TOKEN"]
BLOG_ID        = os.environ["BLOG_ID"]

# ===== ì„¤ì • =====
RSS_URL        = os.environ.get("RSS_URL", "https://rss.blog.naver.com/do_run_.xml")

TOKEN_URL   = "https://oauth2.googleapis.com/token"
BLOGGER_API = "https://www.googleapis.com/blogger/v3"

def log(*a): print(*a, flush=True)

def get_access_token():
    r = requests.post(TOKEN_URL, data={
        "client_id": CLIENT_ID,
        "client_secret": CLIENT_SECRET,
        "refresh_token": REFRESH_TOKEN,
        "grant_type": "refresh_token",
    }, timeout=30)
    r.raise_for_status()
    return r.json()["access_token"]

def auth_headers(at):
    return {"Authorization": f"Bearer {at}", "Content-Type":"application/json; charset=utf-8"}

def blogger_get(url, at, params=None):
    r = requests.get(url, headers=auth_headers(at), params=params, timeout=30)
    r.raise_for_status()
    return r.json()

def blogger_post(url, at, json):
    r = requests.post(url, headers=auth_headers(at), json=json, timeout=30)
    r.raise_for_status()
    return r.json()

def normalize_link(u: str) -> str:
    if not u: return u
    p = urlparse(u)
    p = p._replace(query="", fragment="")
    return urlunparse(p)

def entry_dt(e) -> datetime:
    # feedparserì˜ published_parsed/updated_parsed â†’ datetime(UTC)
    for k in ("published_parsed","updated_parsed"):
        t = e.get(k)
        if t: return datetime(*t[:6], tzinfo=timezone.utc)
    # í´ë°±: ë¬¸ìì—´ íŒŒì‹±
    for k in ("published","updated"):
        s = e.get(k)
        if s:
            try: return dtparse.parse(s).astimezone(timezone.utc)
            except: pass
    return None

def fetch_entries(rss_url):
    feed = feedparser.parse(rss_url)
    if feed.bozo:
        log("[warn] RSS parse error:", feed.bozo_exception)
    return feed.entries or []

def summarize(text, limit=400):
    if not text: return ""
    t = html.unescape(text).strip()
    return (t[:limit].rstrip() + "â€¦") if len(t) > limit else t

def source_marker(link: str) -> str:
    # ë„¤ì´ë²„ ê¸€IDë¥¼ ì¶”ì¶œí•´ ìˆ¨ê¹€ ë§ˆì»¤ ìƒì„±
    base = normalize_link(link)
    post_id = base.split("/")[-1] if "/" in base else base
    return f"source:nblog:{post_id}"

def already_posted(at, blog_id, link):
    # 1ì°¨: ë§ˆì»¤ ê²€ìƒ‰
    marker = source_marker(link)
    url = f"{BLOGGER_API}/blogs/{blog_id}/posts/search"
    js = blogger_get(url, at, params={"q": f'"{marker}"'})
    if js.get("items"): return True
    # 2ì°¨: ë§í¬ ë¬¸ìì—´ ê²€ìƒ‰
    js = blogger_get(url, at, params={"q": f'"{normalize_link(link)}"'})
    return bool(js.get("items"))

def render_content(title, link, summary):
    marker = source_marker(link)
    link_n = normalize_link(link)
    return textwrap.dedent(f"""
        <!-- {marker} -->
        <p><strong>{html.escape(title)}</strong></p>
        <p>{html.escape(summary)}</p>
        <p>ğŸ‘‰ <a href="{html.escape(link_n)}" rel="nofollow noopener" target="_blank">ì›ë¬¸ ë³´ê¸°(ë„¤ì´ë²„ ë¸”ë¡œê·¸)</a></p>
        <hr/>
        <p style="color:#888;font-size:0.9em">ë³¸ í¬ìŠ¤íŠ¸ëŠ” RSS ìë™í™”ë¡œ ë°œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
    """).strip()

def backfill(args):
    at = get_access_token()
    log("[ok] access_token issued")

    entries = fetch_entries(RSS_URL)
    if not entries:
        log("[info] no entries")
        return

    # ê¸°ë³¸: RSSëŠ” ë³´í†µ ìµœì‹ ìˆœ. ì˜¤ë˜ëœ ê²ƒë¶€í„° ì˜¬ë¦¬ë ¤ë©´ ë’¤ì§‘ê¸°
    if args.oldest_first:
        entries = list(reversed(entries))

    # ë‚ ì§œ í•„í„°
    def in_window(e):
        d = entry_dt(e)
        if not d: return True
        ok = True
        if args.since:
            ok = ok and (d >= args.since.replace(tzinfo=timezone.utc))
        if args.until:
            ok = ok and (d <= args.until.replace(tzinfo=timezone.utc))
        return ok

    entries = [e for e in entries if in_window(e)]

    # ì•ìª½ skip
    if args.skip > 0:
        entries = entries[args.skip:]

    # ê°œìˆ˜ ì œí•œ
    if args.max:
        entries = entries[:args.max]

    log(f"[plan] candidates={len(entries)}")

    posted = 0
    for e in entries:
        title = e.get("title") or "(ì œëª© ì—†ìŒ)"
        link  = e.get("link")  or ""
        if not link:
            log(f"[skip] no link: {title}")
            continue

        if not args.force and already_posted(at, BLOG_ID, link):
            log(f"[skip] exists: {normalize_link(link)}")
            continue

        content = render_content(title, link, summarize(e.get("summary") or e.get("description") or ""))

        body = {"kind":"blogger#post", "title": title, "content": content, "labels": ["from-naver"]}
        if args.dry_run:
            log(f"[dry-run] would post: {title}")
        else:
            res = blogger_post(f"{BLOGGER_API}/blogs/{BLOG_ID}/posts/", at, body)
            log(f"[created] {res.get('url')}")
            posted += 1
            time.sleep(2.0)

    log(f"[done] posted={posted}")

def parse_args():
    p = argparse.ArgumentParser(description="Backfill old posts from Naver RSS to Blogger")
    p.add_argument("--max", type=int, default=None, help="ì˜¬ë¦´ ìµœëŒ€ ê°œìˆ˜")
    p.add_argument("--skip", type=int, default=0, help="ì•ì—ì„œ Nê°œ ê±´ë„ˆë›°ê¸°")
    p.add_argument("--since", type=lambda s: dtparse.parse(s), help="ì´ ë‚ ì§œ(í¬í•¨) ì´í›„ (YYYY-MM-DD)")
    p.add_argument("--until", type=lambda s: dtparse.parse(s), help="ì´ ë‚ ì§œ(í¬í•¨) ì´ì „ (YYYY-MM-DD)")
    p.add_argument("--oldest-first", action="store_true", help="ì˜¤ë˜ëœ ê²ƒë¶€í„° ì—…ë¡œë“œ")
    p.add_argument("--force", action="store_true", help="ì´ë¯¸ ì˜¬ë¦° ê¸€ì´ë¼ë„ ë‹¤ì‹œ ì—…ë¡œë“œ")
    p.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ ê³„íšë§Œ")
    return p.parse_args()

if __name__ == "__main__":
    missing = [k for k in ["GCP_CLIENT_ID","GCP_CLIENT_SECRET","GCP_REFRESH_TOKEN","BLOG_ID"] if not os.environ.get(k)]
    if missing:
        raise SystemExit("Missing env: " + ", ".join(missing))
    args = parse_args()
    backfill(args)